{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87404224-b84b-409f-8683-c4a243d29722",
   "metadata": {},
   "source": [
    "# Programming an LLM-based chatbot\n",
    "In this notebook we will program a basic chatbot. We can use multiple large language models, via the [OpenAI Python interface](https://github.com/openai/openai-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752e974d-9aaf-44aa-80fb-01a042cf5774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "openai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903cd45-0b6b-4090-8597-cc4f644cae38",
   "metadata": {},
   "source": [
    "## OpenAI's chatGPT\n",
    "We can use OpenAI's chatGPT via its pay-per-use model by signing up [here](https://openai.com/index/openai-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e686a0f1-9561-426a-b5b8-d222f7966109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_chatGPT(message:str, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"A prompt helper function that sends a message to openAI\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    client = openai.OpenAI()\n",
    "    # todo: enter your API key here:\n",
    "    # client.api_key = \"\"\n",
    "    client.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "    \n",
    "    # submit prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15372ebf-ab4f-446a-8460-6f821499e6da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_chatGPT(\"Hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cb283-fcb3-4dc9-9e0d-a24bc72f0bfd",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Alternatively, we can use [ollama](https://ollama.com/download), a tool that downloads models to our computer and allows us to run them locally. Before executing the following code, you need to run `ollama run gemma:2b` once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abae6df9-543d-4d70-a907-f337872b4f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_ollama(message:str, model=\"gemma:2b\"):\n",
    "    \"\"\"A prompt helper function that sends a message to ollama\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    client = openai.OpenAI()\n",
    "    client.base_url = \"http://localhost:11434/v1\"\n",
    "    client.api_key = \"none\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e514bf8a-74aa-4f7e-83e7-fd6cc82d5719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?\\n\\nWhat would you like to talk about?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ollama(\"Hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87691aff-1a91-4dee-b86c-b6e0f5c7d19c",
   "metadata": {},
   "source": [
    "## Blablador\n",
    "\n",
    "Another alternative is to use the [blablardor](https://helmholtz-blablador.fz-juelich.de/) infrastructure at the Research Center Jülich. Before you can access it, you need to create an API key as explained [on this page](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab55e229-93b9-4e9b-974d-037002690bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_blablador(message:str, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"A prompt helper function that sends a message to Blablador (FZ Jülich)\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # setup connection to the LLM\n",
    "    client = openai.OpenAI()\n",
    "    client.base_url = \"https://helmholtz-blablador.fz-juelich.de:8000/v1\"\n",
    "    \n",
    "    # todo: enter your API key here:\n",
    "    # client.api_key = \"\"\n",
    "    client.api_key = os.environ.get('BLABLADOR_API_KEY')\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7654a20-a307-4b26-8d25-bef20b70224e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_blablador(\"Hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333181f-2c6d-43c2-9318-0e74cde97798",
   "metadata": {},
   "source": [
    "## Decide for a prompt function\n",
    "Here we need to decide for one of the prompt-functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721c3126-67a5-4a65-8bfb-c054bd4dd153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176081e8-c17c-4f15-8b66-c7d1dfc235dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"Hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a5b4c-29d7-4ec7-b7e2-f232a695f960",
   "metadata": {},
   "source": [
    "## Adding memory\n",
    "Our prompt function does not have memory yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b50e242-35c8-4e84-8aa9-a1cfb7516604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Robert, nice to meet you! How can I assist you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"Hi, my name is Robert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daca404f-312b-4966-99a2-2d197391639d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to your personal information and cannot determine your name. You would need to provide me with your name for me to know it.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abdfba2-1abb-4662-8fce-8643e29dfcd3",
   "metadata": {},
   "source": [
    "Memory can be simply a list where we collect questions and answers in a format specified by the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a137f79a-287c-4283-8e66-53c8863cdc86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "def prompt_with_memory(message:str):\n",
    "    \n",
    "    # convert message in the right format and store it in memory\n",
    "    question = {\"role\": \"user\", \"content\": message}\n",
    "    chat_history.append(question)\n",
    "    \n",
    "    # receive answer\n",
    "    response = prompt(chat_history)\n",
    "    \n",
    "    # convert answer in the right format and store it in memory\n",
    "    answer = {\"role\": \"assistant\", \"content\": message}\n",
    "    chat_history.append(answer)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c6e8e1-04eb-43b4-b259-b47f6a0907e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Robert. How can I assist you today?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_memory(\"Hi, my name is Robert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3153f9b3-1de7-4e9d-9e63-ded31362680a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Robert.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_memory(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9a1aa-c600-4695-a5ca-91c40453dee9",
   "metadata": {},
   "source": [
    "## Continuous discussion\n",
    "\n",
    "Finally, we use the functions above for a continuous chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd8aa78-9642-4523-975a-bdf50dc397c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q: hi my name is Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Nice to meet you, Alice! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q: what is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Your name is Alice.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q: How can I denoise an image using Python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: You can denoise an image in Python using libraries such as OpenCV or scikit-image. One common method for denoising images is using the bilateral filter, which preserves edges while smoothing the rest of the image. Here's an example using OpenCV:\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "\n",
      "# Load the image\n",
      "image = cv2.imread('image.jpg')\n",
      "\n",
      "# Denoise the image using the bilateral filter\n",
      "denoised_image = cv2.bilateralFilter(image, 9, 75, 75)\n",
      "\n",
      "# Display the denoised image\n",
      "cv2.imshow('Denoised Image', denoised_image)\n",
      "cv2.waitKey(0)\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "This code will load an image, apply the bilateral filter for denoising, and display the denoised image. You can adjust the parameters of the bilateral filter (like the kernel size and sigma values) to achieve the desired level of denoising.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Goodbye! If you have any more questions in the future, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\n",
    "while(question != \"bye\"):\n",
    "    question = input(\"Q:\")\n",
    "    \n",
    "    answer = prompt_with_memory(question)\n",
    "    print(\"A:\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefdb37-602d-4864-8ab3-f590bc6c8ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
