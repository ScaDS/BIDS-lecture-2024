{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9473c726-2de4-4791-9825-4c28fb4ebc96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple retrieval augmented generation\n",
    "In this notebook we see how retrieval augmented generation (RAG) works using OpenAI and numpy. This implementation avoids using complex libraries intentionally. To keep the code simple, we are using Euclidean distances to determine related entries in the knowledge base. [Maximum inner product search](https://en.wikipedia.org/wiki/Maximum_inner-product_search) is more common in the field though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c88fd7-a683-4417-8090-705924ffde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def show(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0005d-a29f-4234-b40a-8f057aa0d535",
   "metadata": {},
   "source": [
    "We aim to answer this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cee2a87-dde6-4b8d-aafb-a146e2caf561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How can I label objects in an image?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd89ac-bd21-491d-84b8-42a26dd00afd",
   "metadata": {},
   "source": [
    "... using these code snippets (and more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d2d162-3818-480d-bbed-b427d3753b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('code_snippets.txt', 'r') as file:\n",
    "    all_code_snippets = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f201eec1-6ed0-4236-9594-286894574779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Displays an image with a slider and label showing mouse position and intensity.\n",
       "```python\n",
       "stackview.annotate(image, labels)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Allows cropping an image along all axes.\n",
       "```python\n",
       "stackview.crop(image)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Showing an image stored in variable `image` and a segmented image stored in variable `labels` on top. Also works with two images or two label images.\n",
       "```python\n",
       "stackview.curtain(image, labels, alpha: float = 1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = all_code_snippets.split(\"\\n\\n\")\n",
    "[show(s) for s in splits[:3]];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969aa87f-e7b1-49a2-9b1d-451e8d6434ba",
   "metadata": {},
   "source": [
    "## Vector embeddings\n",
    "To make our code snippets searchable, we need to created vector embedding form them, we need to turn them into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1773459-a2ef-4eae-a03d-e2ba1f9cec5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed(text):\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d221a415-03df-4af1-bee0-84b531f3b68b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector = embed(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022b4d1a-d512-4174-8a28-24dd633209da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391a8185-d457-44cc-8ceb-3e014c7e5127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.002119065960869193, -0.04909009113907814, 0.02101006731390953]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a249c-7571-4de9-b860-9c4f24fc1e94",
   "metadata": {},
   "source": [
    "## Vector store\n",
    "We also need a vector store, which is basically just a dictionary that allows us to quickly find a text given a corresponding vector, or a vector that has a short distance to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15a7142-1987-4f12-bbd6-5685a7f0c6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, texts=None):\n",
    "        self._store = {}\n",
    "        if texts is not None:\n",
    "            for text in texts:\n",
    "                self._store[tuple(embed(text))] = text\n",
    "    \n",
    "    def search(self, text, n_best_results=3):\n",
    "        single_vector = embed(text)\n",
    "        \n",
    "        # Step 1: Compute Euclidean distances\n",
    "        distances = [(np.linalg.norm(np.asarray(single_vector) - np.asarray(vector)), vector) for vector in self._store.keys()]\n",
    "\n",
    "        # Step 2: Sort distances and get the three vectors with the shortest distances\n",
    "        distances.sort()  # Sort based on the first element in the tuple (distance)\n",
    "        closest_vectors = [vec for _, vec in distances[:n_best_results]]  # Extract only the vectors\n",
    "\n",
    "        self.distances = distances\n",
    "        \n",
    "        return [self._store[tuple(v)] for v in closest_vectors]\n",
    "    \n",
    "    def get_text(self, vector):\n",
    "        return self._store[vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a321b23-b6bc-44fb-a4c6-0b273f1ec69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectore_store = VectorStore(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39b4fa-596b-4039-97eb-27b02c7fb431",
   "metadata": {},
   "source": [
    "## Searching the vector store\n",
    "We can then search in the store for vectors and corresponding texts that are close by a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7f8d20-21be-4573-ba20-d9ff1557ab74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I label objects in an image?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23771f0-d2db-4f05-9368-198d94f0ab45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004170199856162071, 0.03236572816967964, -0.0011563869193196297]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embed(question)\n",
    "vector[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54f30f2-235d-40dd-8282-8ce794e35ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Labels objects in grey-value images using Gaussian blurs, spot detection, Otsu-thresholding, and Voronoi-labeling from isotropic input images.\n",
       "```python\n",
       "cle.voronoi_otsu_labeling(source: ndarray, label_image_destination: ndarray = None, spot_sigma: float = 2, outline_sigma: float = 2) -> ndarray\n",
       "```\n",
       "\n",
       "* Draw a mesh between close-by objects in a label image:\n",
       "```python\n",
       "mesh = cle.draw_mesh_between_proximal_labels(labels, maximum_distance:int)\n",
       "```\n",
       "\n",
       "\n",
       "* Apply morphological opening operation, fill label gaps with voronoi-labeling, and mask background pixels in label image.\n",
       "```python\n",
       "cle.smooth_labels(labels_input: ndarray, labels_destination: ndarray = None, radius: int = 0) -> ndarray\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "related_code_snippets = vectore_store.search(question)\n",
    "show(\"\\n\\n\".join(related_code_snippets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48968620-c770-4069-82af-e77e4e79ae86",
   "metadata": {},
   "source": [
    "## Prompting OpenAI\n",
    "We will also need access to a large language model (LLM) to combine the code snippets and the question to retrieve an answer to our question that involves the code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48f95c50-a6c8-40c7-8634-e6cb687c7582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_chatGPT(message:str, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"A prompt helper function that sends a message to openAI\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import openai\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    # todo: enter your API key here:\n",
    "    client = openai.OpenAI(api_key = os.environ.get('OPENAI_API_KEY'))\n",
    "    \n",
    "    # submit prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c9e35-0ebb-46a0-a4f6-5fd4f32692aa",
   "metadata": {},
   "source": [
    "We can then assemble code snippets and question to a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13413154-ff5b-4c75-8a38-833d4eb63b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question by the very end and consider given code snippets. \n",
      "Choose at least one of the code-snippets.\n",
      "Only write Python code that answers the question.\n",
      "\n",
      "## Code snippets\n",
      "* Labels objects in grey-value images using Gaussian blurs, spot detection, Otsu-thresholding, and Voronoi-labeling from isotropic input images.\n",
      "```python\n",
      "cle.voronoi_otsu_labeling(source: ndarray, label_image_destination: ndarray = None, spot_sigma: float = 2, outline_sigma: float = 2) -> ndarray\n",
      "```\n",
      "\n",
      "* Draw a mesh between close-by objects in a label image:\n",
      "```python\n",
      "mesh = cle.draw_mesh_between_proximal_labels(labels, maximum_distance:int)\n",
      "```\n",
      "\n",
      "\n",
      "* Apply morphological opening operation, fill label gaps with voronoi-labeling, and mask background pixels in label image.\n",
      "```python\n",
      "cle.smooth_labels(labels_input: ndarray, labels_destination: ndarray = None, radius: int = 0) -> ndarray\n",
      "```\n",
      "\n",
      "## Question\n",
      "How can I label objects in an image?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\\n\".join(related_code_snippets)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the question by the very end and consider given code snippets. \n",
    "Choose at least one of the code-snippets.\n",
    "Only write Python code that answers the question.\n",
    "\n",
    "## Code snippets\n",
    "{context}\n",
    "\n",
    "## Question\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfb432-97e5-40fa-b151-288ed9340983",
   "metadata": {},
   "source": [
    "## Answering our question\n",
    "Eventually we can answer our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4471a899-a6c2-4de0-9a33-6b0731f2140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You can label objects in an image using the `voronoi_otsu_labeling` function from the first code snippet. Here is an example code snippet:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "import pyclesperanto_prototype as cle\n",
       "\n",
       "# Load your image data\n",
       "image = np.array([[0, 0, 0, 0, 0],\n",
       "                   [0, 1, 1, 0, 0],\n",
       "                   [0, 1, 1, 1, 0],\n",
       "                   [0, 0, 1, 0, 0],\n",
       "                   [0, 0, 0, 0, 0]])\n",
       "\n",
       "# Label objects in the image\n",
       "labels = cle.voronoi_otsu_labeling(image)\n",
       "\n",
       "# Display the labeled image\n",
       "print(labels)\n",
       "```\n",
       "\n",
       "This code snippet uses the `voronoi_otsu_labeling` function to label objects in the input image."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = prompt_chatGPT(prompt)\n",
    "\n",
    "show(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690dfe2-82db-471a-85a5-3b3166e88bbd",
   "metadata": {},
   "source": [
    "## Prompting without RAG\n",
    "\n",
    "In comparison, we send the same question together with minimal instructions to ChatGPT without out additional code-snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db88296c-3f2f-4e93-b34c-4f8bafcdc048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You can label objects in an image using image processing techniques such as contour detection and bounding box drawing. Here is an example code using OpenCV library in Python:\n",
       "\n",
       "```python\n",
       "import cv2\n",
       "\n",
       "# Load the image\n",
       "image = cv2.imread('image.jpg')\n",
       "\n",
       "# Convert the image to grayscale\n",
       "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
       "\n",
       "# Apply thresholding to get binary image\n",
       "ret, thresh = cv2.threshold(gray, 127, 255, 0)\n",
       "\n",
       "# Find contours of objects in the image\n",
       "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
       "\n",
       "# Draw bounding boxes around objects\n",
       "for contour in contours:\n",
       "    x, y, w, h = cv2.boundingRect(contour)\n",
       "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
       "\n",
       "# Display the image\n",
       "cv2.imshow('Labeled Image', image)\n",
       "cv2.waitKey(0)\n",
       "cv2.destroyAllWindows()\n",
       "```\n",
       "\n",
       "This code will find the objects in the image, draw bounding boxes around them, and display the labeled image. Make sure to replace 'image.jpg' with the path to your image file."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = prompt_chatGPT(f\"\"\"\n",
    "Write Python code to answer this question:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "show(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f995cc8-8374-46a7-929f-f328a3887732",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Modify the question and ask for drawing a mesh between near neighbors. Prompt chatGPT with and without the RAG-approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742d65a-724f-4601-ace4-3db758805c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
